{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3R4Wr4bX9xJ8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Snow Man by Wallace Stevens (1879-1955)\n",
        "\n",
        "One must have a mind of winter To regard the frost and the boughs Of the pine-trees crusted with snow; And have been cold a long time To behold the junipers shagged with ice, The spruces rough in the distant glitter Of the January sun; and not to think Of any misery in the sound of the wind, In the sound of a few leaves, Which is the sound of the land Full of the same wind That is blowing in the same bare place For the listener, who listens in the snow, And, nothing himself, beholds Nothing that is not there and the nothing that is.\n",
        "\n",
        "create a variation of the N+7 technique we will name P+7. Using the GPT-2 language model, you will replace the last word of each line from The Snow Man with the word that has the seventh-highest probability according to the model’s predictions.\n",
        "# New Section"
      ],
      "metadata": {
        "id": "xPRFp7lw9xux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# P+7"
      ],
      "metadata": {
        "id": "ZcqVtQ8bFVOG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
        "\n",
        "poem = \"\"\"\n",
        "One must have a mind of winter\n",
        "To regard the frost and the boughs\n",
        "Of the pine-trees crusted with snow;\n",
        "And have been cold a long time\n",
        "To behold the junipers shagged with ice,\n",
        "The spruces rough in the distant glitter\n",
        "Of the January sun; and not to think\n",
        "Of any misery in the sound of the wind,\n",
        "In the sound of a few leaves,\n",
        "Which is the sound of the land\n",
        "Full of the same wind\n",
        "That is blowing in the same bare place\n",
        "For the listener, who listens in the snow,\n",
        "And, nothing himself, beholds\n",
        "Nothing that is not there and the nothing that is.\n",
        "\"\"\"\n",
        "\n",
        "# Split the poem into lines\n",
        "lines = poem.strip().split('\\n')\n",
        "\n",
        "# Process each line\n",
        "for line in lines:\n",
        "    # Remove the last word\n",
        "    words = line.split()\n",
        "    line_without_last_word = \" \".join(words[:-1])\n",
        "\n",
        "    # Tokenize the line without the last word\n",
        "    input_ids = tokenizer.encode(line_without_last_word, return_tensors=\"pt\")\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits[:, -1, :]  # Get logits for the last token\n",
        "\n",
        "    # Get the top 10 predictions and their probabilities\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    top_probs, top_indices = torch.topk(probs, k=10)\n",
        "\n",
        "    # Get the 7th most probable word\n",
        "    predicted_token_id = top_indices[0, 6].item()  # Index 6 for the 7th word\n",
        "    predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "    # print(f\"Original line: {line}\")\n",
        "    # print(f\"Line without last word: {line_without_last_word}\")\n",
        "    # print(f\"7th most probable next word: {predicted_token}\")\n",
        "    print(f\"{line_without_last_word} {predicted_token}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53j4stqq9y4M",
        "outputId": "68cf62cb-86bd-46a1-ffdf-f12e9b4167bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One must have a mind of  her\n",
            "\n",
            "To regard the frost and the  death\n",
            "\n",
            "Of the pine-trees crusted with  oil\n",
            "\n",
            "And have been cold a long  way\n",
            "\n",
            "To behold the junipers shagged with  white\n",
            "\n",
            "The spruces rough in the distant  horizon\n",
            "\n",
            "Of the January sun; and not to  have\n",
            "\n",
            "Of any misery in the sound of the  sound\n",
            "\n",
            "In the sound of a few  shots\n",
            "\n",
            "Which is the sound of the  voice\n",
            "\n",
            "Full of the same  day\n",
            "\n",
            "That is blowing in the same bare  air\n",
            "\n",
            "For the listener, who listens in the  morning\n",
            "\n",
            "And, nothing himself,  I\n",
            "\n",
            "Nothing that is not there and the nothing that  isn\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# P+X"
      ],
      "metadata": {
        "id": "lqEREI3QJRKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: write a script that does the same as above, but replaces the last word with the 20th most likely predicted word\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "# The Snow Man by Wallace Stevens (1879-1955)\n",
        "#\n",
        "# One must have a mind of winter To regard the frost and the boughs Of the pine-trees crusted with snow; And have been cold a long time To behold the junipers shagged with ice, The spruces rough in the distant glitter Of the January sun; and not to think Of any misery in the sound of the wind, In the sound of a few leaves, Which is the sound of the land Full of the same wind That is blowing in the same bare place For the listener, who listens in the snow, And, nothing himself, beholds Nothing that is not there and the nothing that is.\n",
        "#\n",
        "# create a variation of the N+7 technique we will name P+7. Using the GPT-2 language model, you will replace the last word of each line from The Snow Man with the word that has the seventh-highest probability according to the model’s predictions.\n",
        "# # New Section\n",
        "\n",
        "\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
        "\n",
        "poem = \"\"\"\n",
        "One must have a mind of winter\n",
        "To regard the frost and the boughs\n",
        "Of the pine-trees crusted with snow;\n",
        "And have been cold a long time\n",
        "To behold the junipers shagged with ice,\n",
        "The spruces rough in the distant glitter\n",
        "Of the January sun; and not to think\n",
        "Of any misery in the sound of the wind,\n",
        "In the sound of a few leaves,\n",
        "Which is the sound of the land\n",
        "Full of the same wind\n",
        "That is blowing in the same bare place\n",
        "For the listener, who listens in the snow,\n",
        "And, nothing himself, beholds\n",
        "Nothing that is not there and the nothing that is.\n",
        "\"\"\"\n",
        "\n",
        "# Split the poem into lines\n",
        "lines = poem.strip().split('\\n')\n",
        "\n",
        "# Process each line\n",
        "for line in lines:\n",
        "    # Remove the last word\n",
        "    words = line.split()\n",
        "    line_without_last_word = \" \".join(words[:-1])\n",
        "\n",
        "    # Tokenize the line without the last word\n",
        "    input_ids = tokenizer.encode(line_without_last_word, return_tensors=\"pt\")\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits[:, -1, :]  # Get logits for the last token\n",
        "\n",
        "    # Get the top 20 predictions and their probabilities\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    top_probs, top_indices = torch.topk(probs, k=200)\n",
        "\n",
        "    # Get the 20th most probable word\n",
        "    predicted_token_id = top_indices[0, 44].item()  # Index 44 for the 45th word\n",
        "    predicted_token = tokenizer.decode([predicted_token_id])\n",
        "\n",
        "    # print(f\"Original line: {line}\")\n",
        "    # print(f\"Line without last word: {line_without_last_word}\")\n",
        "    # print(f\"20th most probable next word: {predicted_token}\")\n",
        "    print(f\"{line_without_last_word} {predicted_token}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zVFCM44DIOe",
        "outputId": "ac3aef4a-7b7a-4447-a3af-4d476ffc5889"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One must have a mind of  awe\n",
            "\n",
            "To regard the frost and the  icy\n",
            "\n",
            "Of the pine-trees crusted with  powdered\n",
            "\n",
            "And have been cold a long  fucking\n",
            "\n",
            "To behold the junipers shagged with  b\n",
            "\n",
            "The spruces rough in the distant  darkness\n",
            "\n",
            "Of the January sun; and not to  ask\n",
            "\n",
            "Of any misery in the sound of the  city\n",
            "\n",
            "In the sound of a few  voice\n",
            "\n",
            "Which is the sound of the  storm\n",
            "\n",
            "Full of the same  spirit\n",
            "\n",
            "That is blowing in the same bare  ground\n",
            "\n",
            "For the listener, who listens in the  upper\n",
            "\n",
            "And, nothing himself,  everything\n",
            "\n",
            "Nothing that is not there and the nothing that  are\n",
            "\n"
          ]
        }
      ]
    }
  ]
}